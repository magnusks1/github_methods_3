---
title: "practical_exercise_5, Methods 3, 2021, autumn semester"
author: 'Anja, Astrid, Jessica, Juli & Magnus'
date: "27 okt 2021"
output: pdf_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readbulk, lme4, interactions, multcomp, GMCM, ggpubr)
```

# Exercises and objectives
The objectives of the exercises of this assignment are based on: https://doi.org/10.1016/j.concog.2019.03.007  
  
4) Download and organise the data from experiment 1  
5) Use log-likelihood ratio tests to evaluate logistic regression models  
6) Test linear hypotheses  
7) Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is part 2 of Assignment 2 and will be part of your final portfolio


# EXERCISE 4

Download and organise the data from experiment 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 1 (there should be 29).  
The data is associated with Experiment 1 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  
  
## 4.1)  
Put the data from all subjects into a single data frame - note that some of the subjects do not have the _seed_ variable. For these subjects, add this variable and make in _NA_ for all observations. (The _seed_ variable will not be part of the analysis and is not an experimental variable)  
```{r, message=FALSE} 
df <- read_bulk('experiment_1/')
```

### 4.1.i. 
Factorise the variables that need factorising  

```{r}

glimpse(df)

df$trial.type <- as.factor(df$trial.type)
  # The staircase trial types were only introduced in experiment 2 and is an 
  # adaptive procedure that allows to collect more data at the threshold 
  # visibility. It is not explained what a staircase procedure actually is, but it 
  # is used at the beginning of the study before collecting the actual trials.The 
  # other trials are experiment trials.

df$pas <- as.factor(df$pas)
  # The Perceptual Awareness Scale, ranging from 1-4.
  # No Experience (NE) -> 1
  # Weak Glimpse (WG) -> 2
  # Almost Clear Experience (ACE) -> 3
  # Clear Experience (CE) -> 4

# trial
  # The text said that each participant performed 864 experiment trials and 18 
  # practice trials beforehand. In the data, it seems that 431   trials are recorded 
  # for each participant and a varying number of staircase trials

# jitter.x
# jitter.y
# odd.digit

# target.contrast
  # The opacity of the target relative to the background, it was adjusted to 
  # match the threshold of each participant

df$target.frames <- as.integer(df$target.frames)
  # the length of time (in frames) the target was shown to participants

df$cue <- as.factor(df$cue)
  # A cue of the possible number of digits presented for each trial, i.e. a kind 
  # of framing. Repeated 12 times before a new cue was used

df$task <- as.factor(df$task)
  # The 3 types of tasks: 
  # Singles
  # Pairs
  # Quadruplets 

df$target.type <- as.factor(df$target.type)
  # Whether the target was odd or even

# rt.subj 
  # Reaction time of the subjective response

# even.digit
# seed

df$rt.obj <- as.numeric(df$rt.obj)
  # Reaction time of the objective response

df$obj.resp <- as.factor(df$obj.resp)
  # The subjects answer to whether the number was odd or even

df$subject <- as.factor(df$subject)
  # ParticipantID

# File

```
    
### 4.1.ii. 
Remove the practice trials from the dataset (see the _trial.type_ variable)  
```{r}

df <- df %>% 
  filter(trial.type != "practice")

```
    
    
### 4.1.iii. 
Create a _correct_ variable  
```{r}

df$correct <- ifelse(df$obj.resp == "o" & df$target.type == "odd" | 
                       df$obj.resp == "e" & df$target.type == "even", 1, 0)

class(df$correct)
df$correct <- as.logical(df$correct)

```
    
    
### 4.1.iv. 
Describe how the _target.contrast_ and _target.frames_ variables differ compared to the data from part 1 of this assignment  

> \textcolor{blue}{Target contrast is the same value for all: 0.1. In experiment 2 it varied between 1 and 0.01000000. Explains how opaque the target number was.
Target frames varies between 1 and 6, in experiment 2 they were all 3. Explains for how many frames (11.8 ms per frame) the target was shown}

# EXERCISE 5 
Use log-likelihood ratio tests to evaluate logistic regression models

## 5.1) 
Do logistic regression - _correct_ as the dependent variable and _target.frames_ as the independent variable. (Make sure that you understand what _target.frames_ encode). Create two models - a pooled model and a partial-pooling model. The partial-pooling model should include a subject-specific intercept.  
```{r}
m1.pool <- glm(correct ~ target.frames, df, family = "binomial")
m1.part <- glmer(correct ~ target.frames + (1 | subject), df, family = "binomial") 
```


### 5.1.i.
The likelihood-function for logistic regression is: $L(p)={\displaystyle\prod_{i=1}^Np^{y_i}(1-p)^{(1-y_i)}}$ (Remember the probability mass function for the Bernoulli Distribution). Create a function that calculates the likelihood.
```{r}
lik.fun <- function(model, y){
  p <- fitted(model) # estimated y-values
  y <- y # actual y values
  
  return(prod(p^(y)*(1-p)^(1-y)))
}
```
    
### 5.1.ii. 
The log-likelihood-function for logistic regression is: $l(p) = {\displaystyle\sum_{i=1}^N}[y_i\ln{p}+(1-y_i)\ln{(1-p)}$. Create a function that calculates the log-likelihood  
```{r}
loglik.fun <- function(model, y){
  p <- fitted(model) # estimated y-values
  y <- y # actual y values
  
  return(sum(y*log(p)+(1-y)*log(1-p)))
}
```

### 5.1.iii. 
Apply both functions to the pooling model you just created. Make sure that the log-likelihood matches what is returned from the _logLik_ function for the pooled model. Does the likelihood-function return a value that is surprising? Why is the log-likelihood preferable when working with computers with limited precision?  
```{r}
lik.fun(m1.pool, df$correct)
loglik.fun(m1.pool, df$correct)
logLik(m1.pool)
```
> \textcolor{blue}{When checking our function with the logLik, the same numbers are returned as with our loglik function. The difference is that the R-log-likelihood-function returns degrees of freedom which our model does not.
The likelihood-function returns a value of 0 which is surprising. There are no decimals — so the number is probably just extremely small and not entirely 0. And furthermore, this computer is not precise enough to show the exact value.} 


### 5.1.iv. 
Now show that the log-likelihood is a little off when applied to the partial pooling model - (the likelihood function is different for the multilevel function - see section 2.1 of https://www.researchgate.net/profile/Douglas-Bates/publication/2753537_Computational_Methods_for_Multilevel_Modelling/links/00b4953b4108d73427000000/Computational-Methods-for-Multilevel-Modelling.pdf if you are interested)  
```{r}
loglik.fun(m1.part, df$correct)
logLik(m1.part)
```
> \textcolor{blue}{The log-likelihood function returns a higher number for the partial pooling model.}

## 5.2) 
Use log-likelihood ratio tests to argue for the addition of predictor variables, start from the null model, `glm(correct ~ 1, 'binomial', data)`, then add subject-level intercepts, then add a group-level effect of _target.frames_ and finally add subject-level slopes for _target.frames_. Also assess whether or not a correlation between the subject-level slopes and the subject-level intercepts should be included.
```{r}

# start from the null model
m2 <- glm(correct ~ 1, family = binomial, df)

# add subject-level intercepts
m3 <- glmer(correct ~ 1 + (1 | subject), family = binomial, df)

# add a group-level effect of _target.frames_
m4 <- glmer(correct ~ target.frames + (1 | subject), family = binomial, df)

# add subject-level slopes for _target.frames_ (forcing covariance to 0 with ||)
m5 <- glmer(correct ~ target.frames + (target.frames || subject), family = binomial, df)

# model correlation between the subject-level slopes and the subject-level intercepts
m6 <- glmer(correct ~ target.frames + (target.frames | subject), family = binomial, df)

```


### 5.2.i. 
Write a short methods section and a results section where you indicate which model you chose and the statistics relevant for that choice. Include a plot of the estimated group-level function with `xlim=c(0, 8)` that includes the estimated subject-specific functions.

```{r}
text <- c("m2", "m3", "m4", "m5", "m6")
formula <- c("correct ~ 1", "correct ~ 1 + (1 | subject)", "(correct ~ target.frames + (1 | subject)", "(correct ~ target.frames + (target.frames || subject)", "(correct ~ target.frames + (target.frames | subject)" )
logLik <- c(logLik(m2),logLik(m3),logLik(m4),logLik(m5),logLik(m6))
as_tibble(cbind(text, formula, logLik))

df %>% 
  ggplot() + 
  geom_point(aes(x = target.frames, y = fitted(m6), color = subject)) +
  xlim(0, 8)
  
```


> \textcolor{blue}{The best model is (correct ~ target.frames + (target.frames | subject), as it has the logLik value closest to 0. Besides having the best logLik (-10448.83), the model with a correlation between the subject-level slopes and the subject-level intercepts conceptually makes sense because the amount of time you view the target influences subjects differently, they need both an individual slope and intercept. This allows the consideration for individual differences (intercept), and how the increasing length of target frames affects the subjects differently (slope).}

    
### 5.2.ii. 
Also include in the results section whether the fit didn't look good for any of the subjects. If so, identify those subjects in the report, and judge (no statistical test) whether their performance (accuracy) differed from that of the other subjects. Was their performance better than chance? (Use a statistical test this time) (50 %)  
```{r}
df %>% 
  ggplot() + 
  geom_point(aes(x = target.frames, y = fitted(m6), color = subject)) +
  xlim(0, 8) +
  facet_wrap(~subject) +
  theme_bw()
```

> \textcolor{blue}{Subject 24's estimated accuracy seems to increase linearly by target.frames, unlike most other subjects who's estimated accuracy increases exponentially. Other subjects (4, 12, 14) seem to have similar tendencies, although not as noticably as 24. The subjects highest likelihood of being correct is around 0.5, much less than the other subjects.}

```{r}
subject.24 <- df %>% 
  filter(subject == "24") 

mean(subject.24$correct)

t.test((subject.24$correct), mu=0.5) 
```
> \textcolor{blue}{We calculated the mean (average correct responses) for subject 24. This is 57%, using a t-test the very low p-value indicates this is a significant difference from 50% or that the results were up to chance. }

## 5.3) 
Now add _pas_ to the group-level effects - if a log-likelihood ratio test justifies this, also add the interaction between _pas_ and _target.frames_ and check whether a log-likelihood ratio test justifies this  
```{r}
m6.plus.pas <- glmer(correct ~ target.frames + pas + (target.frames | subject), family = binomial, df)
m6.interaction.pas <- glmer(correct ~ target.frames * pas + (target.frames | subject), family = binomial, df)

text <- c("m6", "m6.plus.pas", "m6.interaction.pas")
formula <- c("(correct ~ target.frames + (target.frames | subject)", "(correct ~ target.frames + pas + (target.frames | subject)", "(correct ~ target.frames * pas + (target.frames | subject)")
logLik <- c(logLik(m6),logLik(m6.plus.pas),logLik(m6.interaction.pas))
as_tibble(cbind(text, formula, logLik))

```

 > \textcolor{blue}{log-likelihood ratio test justifies adding pas, and confirms that adding pas as an interaction is the better option}
 
 
### 5.3.i. 
If your model doesn't converge, try a different optimizer  
  
### 5.3.ii.
Plot the estimated group-level functions over `xlim=c(0, 8)` for each of the four PAS-ratings - add this plot to your report (see: 5.2.i) and add a description of your chosen model. Describe how _pas_ affects accuracy together with target duration if at all. Also comment on the estimated functions' behaviour at target.frame=0 - is that behaviour reasonable?  
```{r}
# df %>% ggplot(aes(x = target.frames, y = fitted.values(m6.interaction.pas), color = pas)) + 
#   xlim(c(0,8))+
#   geom_line() +
#   theme_minimal()

d <- interactions::interact_plot(model = m6.interaction.pas, pred = "target.frames", modx = "pas")
d

```



```{r}
d <- df %>% 
  ggplot() + 
  geom_point(aes(x = target.frames, y = fitted(m6.interaction.pas), color = pas)) +
  xlim(0, 8) +
  facet_wrap(~pas) +
  theme_bw()
d
```
 > \textcolor{blue}{The model we chose is (correct ~ target.frames * pas + (target.frames | subject). As seen by the plot, target.frames increases acccuracy, except for pas 1, where the mean accuracy looks similar accros target.frames. Pas also increases accuracy, and thus the combination most likely to give a correct answer is Pas 4 with target.frames 6. The likelihood of being correct at target.frames = 0 is not estimated, this is appropriate as this would mean not showing the subjects the target at all.}
 
 

# EXERCISE 6 
Test linear hypotheses

In this section we are going to test different hypotheses. We assume that we have already proved that more objective evidence (longer duration of stimuli) is sufficient to increase accuracy in and of itself and that more subjective evidence (higher PAS ratings) is also sufficient to increase accuracy in and of itself.  
We want to test a hypothesis for each of the three neighbouring differences in PAS, i.e. the difference between 2 and 1, the difference between 3 and 2 and the difference between 4 and 3. More specifically, we want to test the hypothesis that accuracy increases faster with objective evidence if subjective evidence is higher at the same time, i.e. we want to test for an interaction.  

## 6.1) 
Fit a model based on the following formula: `correct ~ pas * target.frames + (target.frames | subject))`
```{r}
m7 <- glmer(correct ~ pas * target.frames + (target.frames | subject), 
               family = "binomial", df)
```
    
### 6.1.i. 
First, use `summary` (yes, you are allowed to!) to argue that accuracy increases faster with objective evidence for PAS 2 than for PAS 1. 
```{r}
summary(m7)
```
> \textcolor{blue}{Looking at the summary() function for estimated coefficients, we find that PAS 2 increases in accuracy with targetframes 44% faster than PAS 1.
pas2:target.frames  0.44719    0.03475  12.869  < 2e-16 ***}

## 6.2) 
`summary` won't allow you to test whether accuracy increases faster with objective evidence for PAS 3 than for PAS 2 (unless you use `relevel`, which you are not allowed to in this exercise). Instead, we'll be using the function `glht` from the `multcomp` package
```{r}
multcomp::glht(m7)



```

### 6.2.i. 
To redo the test in 6.1.i, you can create a _contrast_ vector. This vector will have the length of the number of estimated group-level effects and any specific contrast you can think of can be specified using this. For redoing the test from 6.1.i, the code snippet below will do.

```{r}
# from 6.1.i: argue that accuracy increases faster with objective evidence for PAS 2 than for PAS 1. 
# -1 is the estimate we want to compare from (baseline). 1 is the estimate we want to compare to.
# Changing what is baseline and what is being compared to is just changing the sign.

contrast.vector <- matrix(c(0, #(Intercept)
                            0, #pas2
                            0, #pas3
                            0, #pas4
                            0, #target.frames
                            1, #pas2:target.frames
                            0, #pas3:target.frames
                            0),#pas4:target.frames
                          nrow=1)

gh <- glht(m7, contrast.vector)
print(summary(gh))

```

### 6.2.ii. 
Now test the hypothesis that accuracy increases faster with objective evidence for PAS 3 than for PAS 2.
```{r}
contrast.vector <- matrix(c(0, #(Intercept)
                            0, #pas2
                            0, #pas3
                            0, #pas4
                            0, #target.frames
                            -1, #pas2:target.frames
                            1,#pas3:target.frames
                            0),#pas4:target.frames
                          nrow=1)

gh <- glht(m7, contrast.vector)
print(summary(gh))
```

### 6.2.iii. 
Also test the hypothesis that accuracy increases faster with objective evidence for PAS 4 than for PAS 3
```{r}
contrast.vector <- matrix(c(0, #(Intercept)
                            0, #pas2
                            0, #pas3
                            0, #pas4
                            0, #target.frames
                            0, #pas2:target.frames
                            -1, #pas3:target.frames
                            1),#pas4:target.frames
                          nrow=1)

gh <- glht(m7, contrast.vector)
print(summary(gh))
```

## 6.3) 
Finally, test that whether the difference between PAS 2 and 1 (tested in 6.1.i) is greater than the difference between PAS 4 and 3 (tested in 6.2.iii)
```{r}
contrast.matrix <- rbind(c(0, 0, 0, 0, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, -1, 1))
rownames(contrast.matrix) <- c("PAS 2-PAS 1", "PAS 4-PAS 3")
gh <- glht(m7, contrast.matrix) 
print(summary(gh))
```

# EXERCISE 7 
Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

We saw in 5.3 that the estimated functions went below chance at a target duration of 0 frames (0 ms). This does not seem reasonable, so we will be trying a different approach for fitting here.  
We will fit the following function that results in a sigmoid, $f(x) = a + \frac {b - a} {1 + e^{\frac {c-x} {d}}}$  
It has four parameters: _a_, which can be interpreted as the minimum accuracy level, _b_, which can be interpreted as the maximum accuracy level, _c_, which can be interpreted as the so-called inflexion point, i.e. where the derivative of the sigmoid reaches its maximum and _d_, which can be interpreted as the steepness at the inflexion point. (When _d_ goes towards infinity, the slope goes towards a straight line, and when it goes towards 0, the slope goes towards a step function).  
  
We can define a function of a residual sum of squares as below

```{r, eval=FALSE}
RSS <- function(data, par)
{
    ## "dataset" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    ## par[1]=a, par[2]=b, par[3]=c, par[4]=d
    a <- par[1]
    b <- par[2]
    c <- par[3]
    d <- par[4]
    
    x <- data$x
    y <- data$y
    
    y.hat <- a + ((b-a) / (1+exp((c-x)/(d))))
   
    RSS <- sum((y - y.hat)^2)
    return(RSS)
}

```


## 7.1) 
Now, we will fit the sigmoid for the four PAS ratings for Subject 7
```{r}
sub7.pas1 <- df %>% 
  filter(subject == "7" & pas == "1") %>% 
  mutate(x = target.frames, y = correct)

sub7.pas2 <- df %>% 
  filter(subject == "7" & pas == "2") %>% 
  mutate(x = target.frames, y = correct)

sub7.pas3 <- df %>% 
  filter(subject == "7" & pas == "3") %>% 
  mutate(x = target.frames, y = correct)

sub7.pas4 <- df %>% 
  filter(subject == "7" & pas == "4") %>% 
  mutate(x = target.frames, y = correct)
```

### 7.1.i. 
Use the function `optim`. It returns a list that among other things contains the four estimated parameters. 
    You should set the following arguments:  
    `par`: you can set _c_ and _d_ as 1. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `fn`: which function to minimise?  
    `data`: the data frame with _x_, _target.frames_, and _y_, _correct_ in it  
    `method`: 'L-BFGS-B'  
    `lower`: lower bounds for the four parameters, (the lowest value they can take), you can set _c_ and _d_ as `-Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `upper`: upper bounds for the four parameters, (the highest value they can take) can set _c_ and _d_ as `Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)
    
```{r}

# for data argument, choose between sub7.pas1, -2, -3 and -4.
par <- c(0.5, 1, 1, 1)
    # a, the minimum accuracy level is set to 0.5 because less than chance level is to be avoided
    # b, the maximum accuracy level is set to 1 because you can't exceed 100% probability
fn <- RSS
method = 'L-BFGS-B'
lower = c(0.5, 0.5, -Inf, -Inf)
    # a, the minimum accuracy level and b the maximum accuracy level is set to 0.5 as we never expect the probability to go below 50%
upper = c(1, 1, Inf, Inf)
    # a, the minimum accuracy level and b the maximum accuracy level is set to 1 as the probaility cant exceed 100%

optim.sub7.pas1 <- optim(par = par, data = sub7.pas1,  fn = fn, method = method, lower = lower, upper = upper)
optim.sub7.pas2 <- optim(par = par, data = sub7.pas2,  fn = fn, method = method, lower = lower, upper = upper)
optim.sub7.pas3 <- optim(par = par, data = sub7.pas3,  fn = fn, method = method, lower = lower, upper = upper)
optim.sub7.pas4 <- optim(par = par, data = sub7.pas4,  fn = fn, method = method, lower = lower, upper = upper)

```
    
### 7.1.ii. 
Plot the fits for the PAS ratings on a single plot (for subject 7) `xlim=c(0, 8)`
```{r}

fit.pas1 <- function(x) optim.sub7.pas1$par[1] + ((optim.sub7.pas1$par[2]-optim.sub7.pas1$par[1]) / (1+exp((optim.sub7.pas1$par[3]-x)/(optim.sub7.pas1$par[4]))))

fit.pas2 <- function(x) optim.sub7.pas2$par[1] + ((optim.sub7.pas2$par[2]-optim.sub7.pas2$par[1]) / (1+exp((optim.sub7.pas2$par[3]-x)/(optim.sub7.pas2$par[4]))))

fit.pas3 <- function(x) optim.sub7.pas3$par[1] + ((optim.sub7.pas3$par[2]-optim.sub7.pas3$par[1]) / (1+exp((optim.sub7.pas3$par[3]-x)/(optim.sub7.pas3$par[4]))))

fit.pas4 <- function(x) optim.sub7.pas4$par[1] + ((optim.sub7.pas4$par[2]-optim.sub7.pas4$par[1]) / (1+exp((optim.sub7.pas4$par[3]-x)/(optim.sub7.pas4$par[4]))))

```

```{r}
a <-  ggplot() +
  xlim(0, 8) +
  ylim(0, 1) +
  geom_function(aes(colour = "pas1"),fun = fit.pas1) +
  geom_function(aes(colour = "pas2"),fun = fit.pas2) +
  geom_function(aes(colour = "pas3"),fun = fit.pas3) +
  geom_function(aes(colour = "pas4"),fun = fit.pas4) +
  labs(x = "target.frames", y = "Likelihood of being correct", title = "Subject 7") +
  theme_minimal()
a
```

### 7.1.iii. 
Create a similar plot for the PAS ratings on a single plot (for subject 7), but this time based on the model from 6.1 `xlim=c(0, 8)´ 

```{r}
new.data <- data.frame(cbind('target.frames' = seq(0, 8, by = 0.001), "pas" = rep(1:4), subject = rep("7")))

new.data$subject <- as.factor(new.data$subject)
new.data$pas <- as.factor(new.data$pas)
new.data$target.frames <- as.numeric(new.data$target.frames)
new.data$est.y <- predict(m7, newdata = new.data, type = "response") 

b <- ggplot(new.data) + 
  geom_line(aes(x = target.frames, y = est.y, color = pas)) +
  xlim(c(0,8)) +
  ylim(c(0,1)) +
  labs(y = "Fitted correct", x = "Target Frames", title = "Subject 7") +
  theme_bw()
b
```

> \textcolor{blue}{We start with creating a new data frame. The 'target frames' column goes from 0-8 increasing by 0.001 given the xlim parameters. The 'pas' column repeats 1-4 in order, 'subject' all for subject 7. Lastly, we use the predict function to create a column of predictied y values based on our model (m7) and this new data frame. The type argument ensures that predictions are as probabilities.}

### 7.1.iv. 
Comment on the differences between the fits - mention some advantages and disadvantages of each way  
```{r}
ggarrange(a,b)
```
> \textcolor{blue}{For the first fit made with our RSS function, the prediction never moves below chance level, as we want it to. This method gives us more control over the parameters of the function. However, it might be problematic to set the parameters manually, especially if you are not a statistics expert, like we are. The second fit based on R's glmer function, the model is more complex, allowing for greater accuracy and flexibility - it is not forced into the rigid sigmoid shape. It is however problematic that the y-values dip below chance level, as this should not be possible.}


## 7.2) 
Finally, estimate the parameters for all subjects and each of their four PAS ratings. Then plot the estimated function at the group-level by taking the mean for each of the four parameters, _a_, _b_, _c_ and _d_ across subjects. A function should be estimated for each PAS-rating (it should look somewhat similar to Fig. 3 from the article:  https://doi.org/10.1016/j.concog.2019.03.007)
```{r}
par <- c(0.5, 1, 1, 1)
fn <- RSS
method = 'L-BFGS-B'
lower = c(0.5, 0.5, -Inf, -Inf)
upper = c(1, 1, Inf, Inf)

loop.df <- df %>% 
  mutate(x = target.frames, 
         y = correct, 
         subject = as.numeric(subject),
         pas = as.numeric(pas))

n <- 0

output <- data.frame(subject=character(),
                 pas=integer(),
                 a=integer(),
                 b=integer(),
                 c=integer(),
                 d=integer())

for (i in 1:29) {
  
  for (n in 1:4) {
  subject.df <- loop.df %>% 
    filter(subject == i & pas == n)
  
  optimated <- optim(par = par, 
                     data = subject.df,  
                     fn = fn, 
                     method = method, 
                     lower = lower, 
                     upper =  upper)
  
  optimated.output <- data.frame(subject=i,
                 pas=n,
                 a=optimated$par[1],
                 b=optimated$par[2],
                 c=optimated$par[3],
                 d=optimated$par[4])
  
  output <- rbind(output, optimated.output)
}
}
```

```{r}
summarised.output <- output %>% 
  group_by(pas) %>% 
  summarise(mean.a=mean(a), mean.b=mean(b), mean.c=mean(c), mean.d=mean(d))
```


```{r}

mean.fit.pas1 <- function(x) summarised.output$mean.a[1] + ((summarised.output$mean.b[1]-summarised.output$mean.a[1]) / (1+exp((summarised.output$mean.c[1]-x)/(summarised.output$mean.d[1]))))

mean.fit.pas2 <- function(x) summarised.output$mean.a[2] + ((summarised.output$mean.b[2]-summarised.output$mean.a[2]) / (2+exp((summarised.output$mean.c[2]-x)/(summarised.output$mean.d[2]))))

mean.fit.pas3 <- function(x) summarised.output$mean.a[3] + ((summarised.output$mean.b[3]-summarised.output$mean.a[3]) / (3+exp((summarised.output$mean.c[3]-x)/(summarised.output$mean.d[3]))))

mean.fit.pas4 <- function(x) summarised.output$mean.a[4] + ((summarised.output$mean.b[4]-summarised.output$mean.a[4]) / (4+exp((summarised.output$mean.c[4]-x)/(summarised.output$mean.d[4]))))

```

```{r}
 c <- ggplot() +
  xlim(0, 8) +
  ylim(0, 1) +
  geom_function(aes(color = "pas1"), fun = mean.fit.pas1) +
  geom_function(aes(color = "pas2"), fun = mean.fit.pas2) +
  geom_function(aes(color = "pas3"), fun = mean.fit.pas3) +
  geom_function(aes(color = "pas4"), fun = mean.fit.pas4) +
  labs(x = "target.frames", y = "Likelihood of being correct", title = "Title") +
  theme_minimal() 
c
```

### 7.2.i. 
Compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.
```{r}
ggarrange(c,d)
```


> \textcolor{blue}{As before, the model based on our RSS has the advantage of always staying above chance level. However the pas 1 function declines with targetframes, which seems counterintuitve. Again, it is problematic that the estimated probability of being correct is less than 50% at target.frames = 0, however this model does not have the issue of pas 1 declining with target.frames}
